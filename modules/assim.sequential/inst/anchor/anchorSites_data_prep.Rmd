---
title: "prep data for anchor sites SDA run"
author: "Dongchen Zhang"
date: '2024-04-25'
output: html_document
---

# setup environment.
## Here we will first load required libraries, enabling the `future` parallel operation, and read the `pecan.xml` settings object and convert it into the `site_info` object containing `site_id`, `latitude`, `longitude`, and `site_name`. Making sure you are using the same column names for `site_id` and `site_name` acorss the entire SDA workflow.

```{r setup, include=FALSE}
library(dplyr)
library(xts)
library(PEcAn.all)
library(purrr)
library(furrr)
library(lubridate)
library(nimble)
library(ncdf4)
library(PEcAnAssimSequential)
library(dplyr)
library(sp)
library(raster)
library(zoo)
library(ggplot2)
library(mnormt)
library(sjmisc)
library(stringr)
library(doParallel)
library(doSNOW)
library(Kendall)
library(lgarch)
library(parallel)
library(googlesheets4)
if (future::supportsMulticore()) {
  future::plan(future::multicore)
} else {
  future::plan(future::multisession)
}
#previous site info
#here we need to create the oldpecan.xml file using the `Create_multi_settings.R` script for the anchor site group on the Bety DB with group ID `1000000033`.
#The reason is that currently the Bety DB is down and it's challenging to create new records within it.
#Therefore, we will need to first grab what we have previously, and iteratively add new sites to the site info and write them into the new pecan.xml file.
setwd("/projectnb/dietzelab/dongchen/anchorSites/")
settings <- PEcAn.settings::read.settings("/projectnb/dietzelab/dongchen/anchorSites/SDA/oldpecan.xml")
pre_site_info <- settings %>% 
  purrr::map(~.x[['run']] ) %>% 
  purrr::map('site')%>% 
  purrr::map(function(site.list){
    #conversion from string to number
    site.list$lat <- as.numeric(site.list$lat)
    site.list$lon <- as.numeric(site.list$lon)
    list(site_id=site.list$id, lat=site.list$lat, lon=site.list$lon, site_name=site.list$name)
  })%>% 
  dplyr::bind_rows() %>% 
  as.list()
```

# Pull new sites (not in Bety) into the current site_info object.
## Currently the BETY database is down and we can only create new sites through the shared Google Sheets. After reading the table, we will first create unique site ids for those who don't have the id. Then, we will combine `site_info` objects from both settings and Google Sheet.

```{r}
#new added site info from google sheet
gs4_deauth()
extra_sites = read_sheet("https://docs.google.com/spreadsheets/d/1n7pVUcrYrB0S8bqrj77tUNrLHA2c_yqzkZL8mgdVDjs/edit#gid=0","To Add")

#create unique ids for NA ids.
total_site_ids <- na.omit(c(pre_site_info$site_id, extra_sites$id))
tempIds <- which(!1:1000 %in% total_site_ids)
extra_sites$id[which(is.na(extra_sites$id))] <- tempIds[1:length(which(is.na(extra_sites$id)))]

#combine those two into a larger site info
for (i in 1:dim(extra_sites)[1]) {
  #remove duplicates
  if (extra_sites$id[i] %in% pre_site_info$site_id) {
    next
  }
  pre_site_info$site_id <- c(pre_site_info$site_id, extra_sites$id[i])
  pre_site_info$lat <- c(pre_site_info$lat, extra_sites$lat[i])
  pre_site_info$lon <- c(pre_site_info$lon, extra_sites$lon[i])
  pre_site_info$site_name <- c(pre_site_info$site_name, extra_sites$sitename[i])
}
```

# Filter the combined sites by NA boundary and land cover types.
## Here we will use the North America Ecoregion map for filtering any location that is beyond the scope. After that, we will use the MODIS land cover product to filter out any location that is identified as Urban, Permanent Snow and Ice, Barren, or Water body.

```{r}
#filter based on NA boundary, land cover.
#boundary
ecoregion.path <- "~/ecoregionMap/"
site_eco <- PEcAn.data.land::EPA_ecoregion_finder(pre_site_info$lat, pre_site_info$lon, ecoregion.path)

bound.ind.rm <- unique(which(is.na(site_eco$L2)))
#land cover
LC <- PEcAn.data.remote::MODIS_LC_prep(site_info = pre_site_info, time_points = settings$state.data.assimilation$start.date)

#tweak LC to actual pfts
LC[which(LC[,2] %in% c("Deciduous Broadleaf Forests", 
                       "Deciduous Needleleaf Forests", 
                       "Mixed Forests")), 2] <- "temperate.deciduous.HPDA"
LC[which(LC[,2] %in% c("Evergreen Broadleaf Forests", 
                       "Evergreen Needleleaf Forests")), 2] <- "boreal.coniferous"
LC[which(LC[,2] %in% c("Closed Shrublands", 
                       "Open Shrublands", 
                       "Woody Savannas", 
                       "Savannas", 
                       "Grasslands",
                       "Permanent Wetlands",
                       "Croplands",
                       "Cropland/Natural Vegetation Mosaics")), 2] <- "semiarid.grassland_HPDA"
LC[which(LC[,2] %in% c("Urban and Built-up Lands", 
                       "Permanent Snow and Ice", 
                       "Barren",
                       "Water Bodies")), 2] <- NA
lc.ind.rm <- which(is.na(LC[,2]))
tot.ind.rm <- sort(unique(c(bound.ind.rm, lc.ind.rm)))
LC <- LC[-tot.ind.rm,]
colnames(LC) <- c("site", "pft")
#write settings and PFT file.
write.csv(LC, file = settings$run$settings.1$inputs$pft.site[[1]], row.names = F)

#write new site info into settings.
site_info <- pre_site_info %>% purrr::map(function(l){
  l <- l[-tot.ind.rm]
  l
})
```

# Write the new site_info object into a new pecan.xml file.
## Here we will first need to setup the `template` variable from the `Create_Multi_settings.R` script within the `/pecan/modules/assim.sequential/inst/MultiSite-Exs/SDA` folder. Then we will create a new settings object based on the filtered site ids. Finally we will need to write other columns of the `site_info` into the settings and write the settings into the `pecan.xml` file.

```{r}
# you will first need to run the Create_Multi_settings script to get the template for the multi-settings
new.settings <- PEcAn.settings::createMultiSiteSettings(templateSettings = template, siteIds = site_info$site_id)

#write Lat and Lon into the settings
for (i in 1:length(site_info$site_id)) {
  temp_ID <- new.settings[[i]]$run$site$id
  index_site_info <- which(site_info$site_id==temp_ID)
  new.settings[[i]]$run$site$lat <- site_info$lat[index_site_info]
  new.settings[[i]]$run$site$lon <- site_info$lon[index_site_info]
  new.settings[[i]]$run$site$name <- site_info$site_name[index_site_info]#temp_ID
}

#write out settings
PEcAn.settings::write.settings(new.settings, outputfile = "newpecan.xml")
```

# Prep the ERA5 data for the sites.
## Here we will first download North America ERA5 files using the `/pecan/modules/data.atmosphere/inst/ERA5/ERA5_NA.download.R` script. After that, once we setup the `in.path` and `out.path` for the ERA5 extraction, we can then run the `PEcAn.data.atmosphere::ERA5_met_process` function for preparing the ERA5 data for each site within the settings.

```{r}
#read settings
settings <- PEcAn.settings::read.settings("/projectnb/dietzelab/dongchen/anchorSites/SDA/newpecan.xml")

#Path for the downloaded ERA5 NC files.
in.path <- "/projectnb/dietzelab/dongchen/anchorSites/ERA5"
#Path for the extracted ERA5 files.
out.path <- "/projectnb/dietzelab/dongchen/anchorSites//ERA5_2012_2021"

# prepare ERA5
paths <- PEcAn.data.atmosphere::ERA5_met_process(settings = settings, in.path = in.path, out.path = out.path, write.db = F, write = T)
```

# Prep observations for SDA (obs.mean and obs.cov).
## Here we will be preparing the observations for the SDA workflow. For preparing the SMAP SMP datasets, we will first need to export the `sites.csv` file and then upload it to the Google Earth Engine (GEE) and use the script specified within the `PEcAn.data.land::SMAP_SMP_prep` script. Once the `SMAP_gee.csv` file has been generated from GEE, we can then operate the `PEcAnAssimSequential::SDA_OBS_Assembler` function for the preparation of the rest data streams.

```{r}
#prepare sites.csv for SMAP_gee.csv download.
settings <- PEcAn.settings::read.settings("/projectnb/dietzelab/dongchen/anchorSites/SDA/pecan.xml")
df <- data.frame()
for (i in seq_along(settings)) {
  df <- rbind(df, list(id = settings[[i]]$run$site$id,
                       lat = as.numeric(settings[[i]]$run$site$lat),
                       lon = as.numeric(settings[[i]]$run$site$lon)))
}
write.csv(df, file = "/projectnb/dietzelab/dongchen/anchorSites/sites.csv", row.names = F)

#prepare observations
obs <- PEcAnAssimSequential::SDA_OBS_Assembler(settings)
```